import os
import django
import random

os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'autograder.settings')
django.setup()

from django.contrib.auth import get_user_model
from classes.models import Class, Enrollment
from assignments.models import Assignment
from submissions.models import SubmissionAttempt, GradebookEntry, TestResult

User = get_user_model()

def run():
    # 1. Get User
    try:
        student = User.objects.get(username='vaibhavi')
    except User.DoesNotExist:
        print("User 'vaibhavi' not found.")
        return

    # 2. Get Class and Assignment
    class_name = 'CSL100-EE'
    assign_id = 'fdeed1fb-a225-43f3-995f-7d7b2521de26'
    
    try:
        cls = Class.objects.get(name=class_name)
        assignment = Assignment.objects.get(id=assign_id)
    except Exception as e:
        print(f"Entities not found: {e}")
        return

    # 3. Define Logic for "Realistic" Code
    # We will use reference_solution for 'success' (perfect score)
    # We will use starter_code + partial logic for 'fail' (low score)
    
    aqs = assignment.assignmentquestion_set.all()
    count = 0
    total_score = 0
    max_score = 0
    
    print("Simulating Vaibhavi's activity...")
    
    # Update Assignment to have max_points = 6 per question
    for aq in aqs:
        aq.custom_points = 6
        aq.save()
    
    submissions_count = 0
    total_score = 0
    
    for aq in aqs:
        q = aq.question
        tags = q.tags or []
        
        # Determine "Student Ability" per topic (Score out of 6)
        # Strong: Strings, Recursion, Dictionaries
        # Weak: Lists, Nested Data
        
        points = 0
        status = 'fail'
        code = ""
        
        if any(t in ['Strings & String Operations', 'Recursion', 'Dictionaries'] for t in tags):
            # Strong: 5 or 6 points
            points = random.choice([5, 6])
            status = 'success' if points == 6 else 'fail' # Strict success?
            if points == 6:
                code = q.reference_solution or f"# Solution for {q.title}\n\ndef solve():\n    pass\n"
            else:
                code = q.reference_solution + "\n# Minor error here..."
            
        elif any(t in ['Lists & List Operations', 'Nested Data Structures'] for t in tags):
            # Weak: 0 to 3 points
            points = random.randint(0, 3)
            status = 'fail'
            code = q.starter_code or ""
            code += f"\n# I tried to implement {q.title} but got stuck.\n# Attempting logic...\n"
            
        else:
            # Average: 2 to 6 points
            points = random.randint(2, 6)
            status = 'success' if points >= 5 else 'fail'
            if points > 4:
                 code = q.reference_solution or f"# Solution for {q.title}\n\n# Implemented logic"
            else:
                 code = q.starter_code + "\n# Work in progress..."

        # Delete existing attempts
        SubmissionAttempt.objects.filter(student=student, assignment_question=aq).delete()
        
        # Create Attempt
        # We set manual_score to 'points' (0-6)
        # Note: GradingInterface shows Score / MaxPoints (6).
        attempt = SubmissionAttempt.objects.create(
            student=student,
            assignment_question=aq,
            status=status,
            attempt_number=1,
            manual_score=points, 
            source_code=code,
            feedback_text=f"Autogenerated feedback: You scored {points}/6."
        )
        
        # Create Test Results to match the points
        # If question has real test cases, use them. If not, mock 6 test cases.
        real_test_cases = q.test_cases or []
        num_tests = len(real_test_cases)
        
        if num_tests == 0:
            # Mock 6 test cases
            for i in range(6):
                # If points=4, pass 4 tests.
                is_pass = i < points
                TestResult.objects.create(
                    attempt=attempt,
                    test_case_id=str(i),
                    status='pass' if is_pass else 'fail',
                    score=1 if is_pass else 0,
                    actual_output="Correct Output" if is_pass else "Error: Unexpected output",
                    error_message="" if is_pass else "AssertionError: 5 != 6"
                )
        else:
            # Usage real test cases
            # We want to match the percentage: points/6
            target_percent = points / 6.0
            num_pass = int(round(target_percent * num_tests))
            
            # Ensure at least 1 fail if status is fail?
            if status == 'fail' and num_pass == num_tests:
                num_pass = num_tests - 1
            if status == 'success' and num_pass < num_tests:
                num_pass = num_tests # Force all pass
                
            for i, tc in enumerate(real_test_cases):
                is_pass = i < num_pass
                TestResult.objects.create(
                    attempt=attempt,
                    test_case_id=str(i),
                    status='pass' if is_pass else 'fail',
                    score=1 if is_pass else 0,
                    actual_output=tc.get('output', 'Expected') if is_pass else "Wrong Output",
                    error_message="" if is_pass else "Mismatch"
                )

        submissions_count += 1
        total_score += points

    # Update Gradebook
    # Final score is average of percentages or total points?
    # GradingInterface shows manual_score / max_points.
    # We'll calculate a percentage for the Gradebook final_score (usually 0-100).
    
    # Total possible points = 30 * 6 = 180
    avg_score_percent = (total_score / (30 * 6)) * 100
    
    entry, _ = GradebookEntry.objects.get_or_create(
        student=student, 
        content_item=assignment
    )
    entry.final_score = avg_score_percent
    entry.points_earned = int(total_score * 10) 
    entry.status = 'graded'
    entry.save()
    
    print(f"Updated Vaibhavi's submissions. Total Score: {total_score}/180 ({avg_score_percent:.1f}%)")

if __name__ == '__main__':
    run()
